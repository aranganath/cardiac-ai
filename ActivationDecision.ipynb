{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils import *\n",
    "from matplotlib import pyplot\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datafiles :  91%|█████████ | 2903/3200 [00:01<00:00, 2462.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Okay, we kind of have a way of loading the data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# We need to collect the data and feed it to the transformer model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Now how do we that ?\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[39m# Torch Tensor data !\u001b[39;00m\n\u001b[1;32m      8\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./Datasets/intracardiac_dataset/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m VmTrainData, pECGTrainData, VmTestData, pECGTestData  \u001b[39m=\u001b[39m fileReaderForActivation(path, \u001b[39m3200\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mData loading from files - complete\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m VmTrainData \u001b[39m=\u001b[39m (VmTrainData \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mmin(VmTrainData))\u001b[39m/\u001b[39m(torch\u001b[39m.\u001b[39mmax(VmTrainData) \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mmin(VmTrainData))\n",
      "File \u001b[0;32m~/gitProjects/cardiac-ai/utils.py:359\u001b[0m, in \u001b[0;36mfileReaderForActivation\u001b[0;34m(path, dataInd)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(pECGData_file, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m--> 359\u001b[0m         pECGTestData\u001b[39m.\u001b[39mappend(get_standard_leads(np\u001b[39m.\u001b[39;49mload(f)))\n\u001b[1;32m    361\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(VmData_file, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    362\u001b[0m         VmTestData\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mload(f))\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-dev/lib/python3.10/site-packages/numpy/lib/npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mopen_memmap(file, mode\u001b[39m=\u001b[39mmmap_mode,\n\u001b[1;32m    430\u001b[0m                                   max_header_size\u001b[39m=\u001b[39mmax_header_size)\n\u001b[1;32m    431\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(fid, allow_pickle\u001b[39m=\u001b[39;49mallow_pickle,\n\u001b[1;32m    433\u001b[0m                                  pickle_kwargs\u001b[39m=\u001b[39;49mpickle_kwargs,\n\u001b[1;32m    434\u001b[0m                                  max_header_size\u001b[39m=\u001b[39;49mmax_header_size)\n\u001b[1;32m    435\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[39m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-dev/lib/python3.10/site-packages/numpy/lib/format.py:776\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    774\u001b[0m version \u001b[39m=\u001b[39m read_magic(fp)\n\u001b[1;32m    775\u001b[0m _check_version(version)\n\u001b[0;32m--> 776\u001b[0m shape, fortran_order, dtype \u001b[39m=\u001b[39m _read_array_header(\n\u001b[1;32m    777\u001b[0m         fp, version, max_header_size\u001b[39m=\u001b[39;49mmax_header_size)\n\u001b[1;32m    778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shape) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    779\u001b[0m     count \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-dev/lib/python3.10/site-packages/numpy/lib/format.py:629\u001b[0m, in \u001b[0;36m_read_array_header\u001b[0;34m(fp, version, max_header_size)\u001b[0m\n\u001b[1;32m    627\u001b[0m     header \u001b[39m=\u001b[39m _filter_header(header)\n\u001b[1;32m    628\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     d \u001b[39m=\u001b[39m safe_eval(header)\n\u001b[1;32m    630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mSyntaxError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    631\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot parse header: \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-dev/lib/python3.10/site-packages/numpy/lib/utils.py:1083\u001b[0m, in \u001b[0;36msafe_eval\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[39m# Local import to speed up numpy's import time.\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mast\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m \u001b[39mreturn\u001b[39;00m ast\u001b[39m.\u001b[39;49mliteral_eval(source)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-dev/lib/python3.10/ast.py:110\u001b[0m, in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[39mreturn\u001b[39;00m left \u001b[39m-\u001b[39m right\n\u001b[1;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[0;32m--> 110\u001b[0m \u001b[39mreturn\u001b[39;00m _convert(node_or_string)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-dev/lib/python3.10/ast.py:84\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m operand\n\u001b[1;32m     83\u001b[0m     \u001b[39mreturn\u001b[39;00m _convert_num(node)\n\u001b[0;32m---> 84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_convert\u001b[39m(node):\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(node, Constant):\n\u001b[1;32m     86\u001b[0m         \u001b[39mreturn\u001b[39;00m node\u001b[39m.\u001b[39mvalue\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "# Okay, we kind of have a way of loading the data\n",
    "# We need to collect the data and feed it to the transformer model\n",
    "# Now how do we that ?\n",
    "\n",
    "\n",
    "# Torch Tensor data !\n",
    "path = './Datasets/intracardiac_dataset/'\n",
    "VmTrainData, pECGTrainData, VmTestData, pECGTestData  = fileReaderForActivation(path, 3200)\n",
    "print('Data loading from files - complete')\n",
    "\n",
    "VmTrainData = (VmTrainData - torch.min(VmTrainData))/(torch.max(VmTrainData) - torch.min(VmTrainData))\n",
    "pECGTrainData = (pECGTrainData - torch.min(pECGTrainData))/(torch.max(pECGTrainData) - torch.min(pECGTrainData))\n",
    "\n",
    "VmTestData = (VmTestData - torch.min(VmTestData))/(torch.max(VmTestData) - torch.min(VmTestData))\n",
    "\n",
    "pECGTestData = (pECGTestData - torch.min(pECGTestData))/(torch.max(pECGTestData) - torch.min(pECGTestData))\n",
    "\n",
    "# Let's just transpose the data\n",
    "pECGTrainData, pECGTestData = pECGTrainData.permute(0,2,1), pECGTestData.permute(0,2,1)\n",
    "\n",
    "print('Normalization - complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2560, 12, 500])\n"
     ]
    }
   ],
   "source": [
    "print(pECGTrainData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "from torch import nn\n",
    "class Network(nn.Module):\n",
    "    def __init__(self,in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.ConvStack = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                      in_channels = in_features, \n",
    "                      out_channels =6, \n",
    "                      kernel_size= 5, \n",
    "                      stride = 3\n",
    "                    ) ,\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(\n",
    "                      in_channels = 6, \n",
    "                      out_channels=3, \n",
    "                      kernel_size = 3, \n",
    "                      stride =2\n",
    "                    ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(\n",
    "                      in_channels = 3,\n",
    "                      out_channels = 1, \n",
    "                      kernel_size = 2, \n",
    "                      stride = 1\n",
    "                    ),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        self.FullyConnectedStack = nn.Sequential(\n",
    "            nn.Linear(in_features=81, out_features=out_features)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.ConvStack(x)\n",
    "        x = self.FullyConnectedStack(x)\n",
    "        return x.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's load the dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = self.X.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "\n",
    "trainloader = CustomDataset(X = pECGTrainData, y = VmTrainData)\n",
    "trainloader = DataLoader(trainloader, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the parameters for the neural network\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "model = Network(in_features=12, out_features=75)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "lossfn = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0         Loss: 0.05101811306708155\n",
      "EPOCH: 1         Loss: 0.05100000634692691\n",
      "EPOCH: 2         Loss: 0.05097875786507235\n",
      "EPOCH: 3         Loss: 0.050953358183414225\n",
      "EPOCH: 4         Loss: 0.05092242431503593\n",
      "EPOCH: 5         Loss: 0.050884058871685525\n",
      "EPOCH: 6         Loss: 0.05083570584004094\n",
      "EPOCH: 7         Loss: 0.050774205037063244\n",
      "EPOCH: 8         Loss: 0.05069658224202267\n",
      "EPOCH: 9         Loss: 0.050606723849467676\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    for inp, op in trainloader:\n",
    "        pred = model(inp.to(device))\n",
    "        loss  = lossfn(op.to(device), pred)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('EPOCH: {epoch}         Loss: {loss}'.format(epoch = epoch, loss=loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "056b7cbeb7cbb58f807a70f9c03cbe05839133636727cde7e87a2d921c04cc10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
